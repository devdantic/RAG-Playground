{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf5530d",
   "metadata": {},
   "source": [
    "# Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7293f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_path = \"D:\\Computer Programming\\Python\\RAG-Playground\\data\\A_Simple_Guide_to_Retrieval_Augmented_Generation_(Abhinav Kimothi)_bibis.ir.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307d3aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Abhinav Kimothi',\n",
      " 'creationDate': \"D:20250617062545+03'30'\",\n",
      " 'creator': 'Adobe InDesign 20.1 (Macintosh)(Foxit Advanced PDF Editor)',\n",
      " 'file_path': 'D:\\\\Computer '\n",
      "              'Programming\\\\Python\\\\RAG-Playground\\\\data\\\\A_Simple_Guide_to_Retrieval_Augmented_Generation_(Abhinav '\n",
      "              'Kimothi)_bibis.ir.pdf',\n",
      " 'format': 'PDF 1.5',\n",
      " 'keywords': '',\n",
      " 'modDate': \"D:20250617062545+03'30'\",\n",
      " 'page': 0,\n",
      " 'producer': 'GPL Ghostscript 9.50',\n",
      " 'source': 'D:\\\\Computer '\n",
      "           'Programming\\\\Python\\\\RAG-Playground\\\\data\\\\A_Simple_Guide_to_Retrieval_Augmented_Generation_(Abhinav '\n",
      "           'Kimothi)_bibis.ir.pdf',\n",
      " 'subject': '',\n",
      " 'title': 'A Simple Guide to Retrieval Augmented Generation',\n",
      " 'total_pages': 258,\n",
      " 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "# Load the documents from the PDF\n",
    "docs = loader.load()\n",
    "\n",
    "# Print a part of the first document and metadata to verify loading\n",
    "docs[0]\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac29aea",
   "metadata": {},
   "source": [
    "# Chunking and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af68726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 740\n",
      "M A N N I N G\n",
      " Abhinav Kimothi\n",
      "Retrieval Augmented\n",
      "Generation\n",
      "A SIMPLE GUIDE TO\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"Number of text chunks: {len(texts)}\")\n",
    "# Print a part of the first text chunk to verify splitting\n",
    "print(texts[0].page_content[:500])  # Print first 500 characters of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e93e60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=200,chunk_overlap=200): # Kept small due to low memory\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde9dbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 740 documents into 8702 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: M A N N I N G\n",
      " Abhinav Kimothi\n",
      "Retrieval Augmented\n",
      "Generation\n",
      "A SIMPLE GUIDE TO...\n",
      "Metadata: {'source': 'D:\\\\Computer Programming\\\\Python\\\\RAG-Playground\\\\data\\\\A_Simple_Guide_to_Retrieval_Augmented_Generation_(Abhinav Kimothi)_bibis.ir.pdf', 'file_path': 'D:\\\\Computer Programming\\\\Python\\\\RAG-Playground\\\\data\\\\A_Simple_Guide_to_Retrieval_Augmented_Generation_(Abhinav Kimothi)_bibis.ir.pdf', 'page': 0, 'total_pages': 258, 'format': 'PDF 1.5', 'title': 'A Simple Guide to Retrieval Augmented Generation', 'author': 'Abhinav Kimothi', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 20.1 (Macintosh)(Foxit Advanced PDF Editor)', 'producer': 'GPL Ghostscript 9.50', 'creationDate': \"D:20250617062545+03'30'\", 'modDate': \"D:20250617062545+03'30'\", 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "chunks=split_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48815e13",
   "metadata": {},
   "source": [
    "# Building Vector Store and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50759988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "import os\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ee31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"\n",
    "    Handles document embedding generation using SentenceTransformer\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initializes the embedding model.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "            \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "            try:\n",
    "                print(f\"Loading embedding model: {self.model_name}\")\n",
    "                self.model = SentenceTransformer(self.model_name)\n",
    "                print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model {self.model_name}: {e}\")\n",
    "                raise\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generates embeddings for a list of texts.\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        try:\n",
    "            print(f\"Generating embeddings for {len(texts)} texts.\")\n",
    "            embeddings = np.array(\n",
    "            self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True, device='cuda' if self.model.device.type == 'cuda' else 'cpu')\n",
    "            )\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d00f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x20c8c289d10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c63915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Manages a ChromaDB vector store for document embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, collection_name: str = \"pdf_docs\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initializes the ChromaDB client and collection.\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"\n",
    "        Initializes the ChromaDB client and collection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client if not exists\n",
    "            print(f\"Initializing ChromaDB client with persistence at: {self.persist_directory}\")\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Create or get collection\n",
    "            try:\n",
    "                self.collection = self.client.get_or_create_collection(\n",
    "                    name=self.collection_name,\n",
    "                    metadata={\"description\": \"PDF Document Embeddings\"}\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"get_or_create_collection failed or returned None: {e}\")\n",
    "                self.collection = None\n",
    "\n",
    "            # If collection is None, try to create one explicitly\n",
    "            if self.collection is None:\n",
    "                try:\n",
    "                    print(f\"Attempting to create collection '{self.collection_name}' explicitly.\")\n",
    "                    self.collection = self.client.create_collection(\n",
    "                        name=self.collection_name,\n",
    "                        metadata={\"description\": \"PDF Document Embeddings\"}\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to create collection: {e}\")\n",
    "                    raise\n",
    "\n",
    "            print(f\"Collection '{self.collection_name}' initialized successfully.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def add_documents(self, docs: List[Any], embeddings: np.ndarray):\n",
    "        if len(docs) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        print(f\"Adding {len(docs)} documents to the vector store.\")\n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "        ids, metadatas, document_texts, embeddings_list = [], [], [], []\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = np.expand_dims(embeddings, axis=0)\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(docs, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            document_texts.append(doc.page_content)\n",
    "\n",
    "            if isinstance(embedding, np.ndarray):\n",
    "                embeddings_list.append(embedding.tolist())\n",
    "            else:\n",
    "                raise TypeError(f\"Expected embedding to be ndarray, got {type(embedding)}\")\n",
    "            \n",
    "\n",
    "        try:\n",
    "            batch_size = 500\n",
    "            for i in range(0, len(ids), batch_size):\n",
    "                end = i + batch_size\n",
    "                self.collection.add(\n",
    "                    ids=ids[i:end],\n",
    "                    embeddings=embeddings_list[i:end],\n",
    "                    metadatas=metadatas[i:end],\n",
    "                    documents=document_texts[i:end],\n",
    "                )\n",
    "                print(f\"Added batch {i // batch_size + 1} ({end} docs)\")\n",
    "\n",
    "            print(f\"Successfully added {len(docs)} documents to vector store\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd70accc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB client with persistence at: ../data/vector_store\n",
      "Collection 'pdf_docs' initialized successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x20c8c53cf90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = VectorStore()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a9160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 8702 texts.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb67417f05b47809db5839d2d0eadf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6958fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 8702 documents to the vector store.\n",
      "Embeddings shape: (8702, 384)\n"
     ]
    }
   ],
   "source": [
    "# Add the documents and embeddings to the vector store\n",
    "vector_store.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863c6e57",
   "metadata": {},
   "source": [
    "# Retrieval Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23786a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "\n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "\n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "\n",
    "            return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vector_store,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever.retrieve(\"What is retrieval augmentation generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489204c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api = os.getenv('GROQ_API')\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=groq_api,\n",
    "    verbose=True,\n",
    "    model_name = \"llama-3.1-8b-instant\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91914dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG Function\n",
    "\n",
    "def rag_simple(query, retreiver, top_k = 5):\n",
    "  results = retreiver.retrieve(query, top_k)\n",
    "  context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "  if not context:\n",
    "    return \"No relevant context found\"\n",
    "\n",
    "  prompt = f\"\"\"use the following context to answer the question correctly\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question:\n",
    "            {query}\n",
    "            \"\"\"\n",
    "\n",
    "  response = llm.invoke([prompt.format(context = context, query = query)])\n",
    "  return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd588e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_simple(\"What is retrieval augmentation generation\", retreiver=rag_retriever)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advanced(query, retriever, top_k = 5, min_score = 0.2, return_context = False):\n",
    "  \"\"\"\n",
    "  RAG Pipeline with some extra features\n",
    "  \"\"\"\n",
    "  results = retriever.retrieve(query, top_k = top_k, score_threshold=min_score)\n",
    "  if not results:\n",
    "    return \"No relevant answer found from the document\"\n",
    "  else:\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source' : doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page' : doc['metadata'].get('page', 'unknown'),\n",
    "        'score' : doc['similarity_score'],\n",
    "        'preview' : doc['content'][:100] + \".....\"\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "\n",
    "    prompt = f\"\"\"use the following context to answer the question correctly\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question:\n",
    "            {query}\n",
    "    \"\"\"\n",
    "    response = llm.invoke([prompt.format(context = context, query = query)])\n",
    "    output = {\n",
    "        'answer' : response.content,\n",
    "        'sources' : sources,\n",
    "        'confidence' : confidence\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "      output['context'] = context\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_advanced(\"What is retrieval augmentation generation\", retriever=rag_retriever, return_context=True)\n",
    "\n",
    "answer['answer']\n",
    "answer['sources']\n",
    "answer['confidence']\n",
    "answer['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761c395",
   "metadata": {},
   "source": [
    "# Adding Agent in the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "client = vector_store.client\n",
    "collection = vector_store.collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc35927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.tools.retriever import create_retriever_tool\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vec_store = Chroma(\n",
    "    client=client,\n",
    "    collection_name=vector_store.collection_name,\n",
    "    persist_directory=vector_store.persist_directory\n",
    ")\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    vec_store.as_retriever(),\n",
    "    \"pdf_document_retriever\",\n",
    "    \"Searches and returns information regarding the RAG document.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51680440",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcfab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "\n",
    "from langchain_classic import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatGroq(\n",
    "    groq_api_key=groq_api,\n",
    "    verbose=True,\n",
    "    model_name = \"llama-3.1-8b-instant\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens = 1024\n",
    "    )\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatGroq(\n",
    "    groq_api_key=groq_api,\n",
    "    verbose=True,\n",
    "    model_name = \"llama-3.1-8b-instant\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens = 1024\n",
    "    )\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatGroq(\n",
    "    groq_api_key=groq_api,\n",
    "    verbose=True,\n",
    "    model_name = \"llama-3.1-8b-instant\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens = 1024\n",
    "    )\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatGroq(\n",
    "    groq_api_key=groq_api,\n",
    "    verbose=True,\n",
    "    model_name = \"llama-3.1-8b-instant\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens = 1024\n",
    "    )\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "print(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\").pretty_print()  # Show what the prompt looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a71433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80905b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24044dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Can you explain what exactly is mentioned about Retrieval Augmented Generation\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
